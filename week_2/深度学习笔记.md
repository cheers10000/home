# 深度学习

## 人工神经网络基础知识

### 机器学习简介

定义:机器学习是使用算法分析数据，==从数据中学习==，然后对新数据做出确定或预测的做法

机器学习与传统编程的区别：

1. 传统编程需要人为制定判断的根据，即给定示例列表，并利用条件语句来分析数据和处理情况
2. 机器学习从数据中学习判断和分类的特征，并将其应用在分析过程中，来学习传统编程所依赖的判断根据

### 深度学习简介

定义:深度学习是机器学习的一个子领域，它使用受大脑神经网络结构和功能启发的算法

### 神经网络架构

==*人工*神经网络 （ANN）==(并不是真正的生物神经网络，只是与生物神经网络共享一些特征)

- 人工神经网络是使用我们所谓的神经元构建的
- ANN 中的神经元被组织成我们所说的**层**
- 层 *在* ANN 中（除输入和输出层之外的所有层）称为**隐藏层**
- 如果 ANN 具有多个隐藏层，则称该 ANN 为**深度 ANN**
- 又可称之为**网、神经网络、型**(*net*、*network* 、*model* )

定义:人工神经网络是一种计算系统，它由一组称为神经元的连接单元组成，这些单元被组织成我们所说的层

==注意:==

- 连接的神经单元形成所谓的网络。神经元之间的每个连接都会将信号从一个神经元传输到另一个神经元。接收神经元处理信号并将信号发送到网络内连接到它的下游神经元。神经元通常也被称为 ==节点==，节点被组织成我们所说的层。
- 在最高级别，每个 ANN 中有三种类型的==层==：
  - ==输入层==(输入数据的每个组件对应一个节点，如果是两个节点，那么这个网络的每个输入都必须有两个维度)
  - ==隐藏层==(为每个隐藏层任意选择的节点数)
  - ==输出层==(每个可能的所需输出对应一个节点，如果是两个节点，那么通过网络向前传递（从左到右）的每个输入都有两个可能的输出，而输出类也称为预测类)
- 数据从输入层开始流经网络，然后穿过隐藏层，直到到达输出层。这称为通过网络的正向传递。位于输入和输出图层之间的图层称为隐藏层

![](Users/0.0/Desktop/屏幕截图_27-3-2025_18834_deeplizard.com.jpeg)

**使用 Keras 在代码中构建 ANN**

在 Keras 中构建所谓的 Sequential 模型并 将 Sequential 模型定义为线性层的 SequenceStack ,这个顺序模型是 Keras 对人工神经网络的实现

```python
#导入所需的 Keras 类
from keras.models import Sequential 
from keras.layers import Dense, Activation
model = Sequential(layers) # 创建一个名为 model 的实例，并将其设置为等于 Sequential 对象
# 密集对象，这些对象中的每一个都称为密集实际上是层
layers = [
    Dense(units=3, input_shape=(2,), activation='relu'), 
    Dense(units=2, activation='softmax')
]
# 传递给每层中的 dense layer 构造函数告诉我们它应该有多少个神经元
# 输入 shape 参数 input_shape=（2，） 告诉我们输入层有多少个神经元
# activation='relu' activation='softmax' 为激活函数的参数
```

==注意==：

-  密集是ANN 中最基本的层类型，并且 dense 层的每个输出都是使用该层的每个输入计算的
- 因为隐藏层中的每个节点都连接到输出层中的所有节点，所以图像中的输出层是一个密集层。隐藏层同样
- 激活函数是一个非线性函数，通常遵循密集层
- 在神经网络编程中，每个隐藏层内的节点数不等于*输入数据中存在的组件数* 

==层的分类==：

- Dense (or fully connected) layers
  密集（或完全连接）层(将每个输入完全连接到其层中的每个输出)
- Convolutional layers  卷积层(通常用于处理图像数据的模型)
- Pooling layers  池化层
- Recurrent layers  循环层(用于处理时间序列数据的模型)
- Normalization layers  归一化层

例子分析：

![](Users/0.0/Desktop/2.jpeg)

1. 第一层，即输入层，由八个节点组成。该层中的 8 个节点都代表数据集中给定样本中的单个特征
2. 数据集中的单个样本由 8 个维度组成。当我们从数据集中选择一个样本并将该样本传递给模型时，样本中包含的八个值中的每一个都将提供给输入层中的相应节点。
3. 八个 input 节点中的每一个都连接到下一层中的每个节点

#### Layer weights  图层权重

定义：两个节点之间的每个连接都有一个关联的权重(一个数字)，表示两个节点之间的连接强度

过程：

1. 当网络在输入层的给定节点收到输入时，此输入将通过连接传递到下一个节点，并且输入将乘以分配给该连接的权重
2. 对于第二层中的每个节点，将计算每个传入连接的加权和
3. 此 sum 将传递给激活函数，该函数对给定的 sum 执行某种类型的转换。例如，激活函数 可以将总和转换为介于 0 和 1 之间的数字(非线性转换)。实际的转换将根据使用的激活函数而有所不同

注意：在神经网络中，特定节点的输出取决于输入的加权和

#### Forward pass through a neural network 通过神经网络的正向传递

定义：对于数据集中的给定样本，从输入层到输出层的整个过程称为通过网络的正向传递

过程：

1. 一旦我们获得给定节点的输出，获得的输出就是作为输入传递给下一层中的节点的值
2. 此过程一直持续到到达输出层。输出层中的节点数取决于我们拥有的可能的输出或预测类的数量。在我们的示例中，我们有四个可能的预测类
3. 设我们的模型的任务是对四种类型的动物进行分类。输出层中的每个节点都表示四种可能性之一。例如，我们可以有猫、狗、美洲驼或蜥蜴。类别或类取决于我们的数据集中有多少类

#### Finding the optimal weights 查找最佳权重

随着模型的学习，所有连接的权重都会更新和优化，以便输入数据点映射到正确的输出预测类

**使用 Keras 在代码中定义神经网络**

```python
from keras.models import Sequential
from keras.layers import Dense, Activation

layers = [
    Dense(units=6, input_shape=(8,), activation='relu'), # 隐藏层
    Dense(units=6, activation='relu'),
    Dense(units=4, activation='softmax') # 输出层
]

model = Sequential(layers)
```

### 神经网络中的激活函数 

定义：在人工神经网络中，激活函数是将节点的输入映射到其相应输出的函数，将总和转换为通常介于某个下限和某个上限之间的数字。此转换通常是非线性转换

==节点输出 = 激活（输入的加权和）==

#### Sigmoid(∈[0,1])

![image-20250327194253446](Users/0.0/AppData/Roaming/Typora/typora-user-images/image-20250327194253446.png)

![image-20250327194322518](Users/0.0/AppData/Roaming/Typora/typora-user-images/image-20250327194322518.png)

==注意==：

神经元可以在0 和 1 之间 ，并且越接近 1 ，神经元的激活程度就越高，越积极，而神经元 0 的激活度就越低

#### Relu

![image-20250327194714847](Users/0.0/AppData/Roaming/Typora/typora-user-images/image-20250327194714847.png)

**线性函数**

且仅当满足以下条件时，该函数 f 才称为线性函数：

![image-20250327194818317](Users/0.0/AppData/Roaming/Typora/typora-user-images/image-20250327194818317.png)

==激活函数意义==：

1. 线性函数的一个重要特征是两个线性函数的组合也是一个线性函数。这意味着，即使在非常深入的神经网络中，如果我们在前向传递期间只对数据值进行线性转换，那么我们网络中从输入到输出的学习映射也将是线性的
2. 我们旨在通过深度神经网络学习的映射类型比简单的线性映射更复杂
3. 大多数激活函数都是非线性的。具有非线性激活函数使我们的神经网络能够计算任意复杂的函数

**使用 Keras 的代码中的激活函数**

```python
from keras.models import Sequential
from keras.layers import Dense, Activation
# 指定激活函数的第一种方法是在层的构造函数中
model = Sequential([
    Dense(units=5, input_shape=(3,), activation='relu')
])
# 第二种方法是在模型实例化后将层和激活函数添加到我们的模型中
model = Sequential()
model.add(Dense(units=5, input_shape=(3,)))
model.add(Activation('relu'))

```

### 训练神经网络

当我们训练模型时，我们基本上是在尝试解决一个优化问题。我们的任务是找到最准确地将输入数据映射到正确输出类的权重。

#### Optimization algorithm  优化算法

1. 权重使用我们所谓的优化算法进行优化。优化过程取决于所选的优化算法。我们还使用术语 *optimizer* 来引用所选算法。最广为人知的优化器称为 *随机梯度下降* ，或者更简单地说，SGD
2. SGD 的目标是最小化我们称为 *loss 函数* 。因此，SGD 以使此损失函数尽可能接近其最小值的方式更新模型的权重

#### Loss function  损失函数

损失是网络对图像的预测与图像的真实标签之间的误差或差异，SGD 将尝试最小化此误差，以使我们的模型在预测中尽可能准确

  常见的损失函数是 *均方误差* （MSE）

通过我们的模型传递所有数据后，我们将继续一遍又一遍地传递相同的数据。这种通过网络重复发送相同数据的过程被认为是 ***培训***

例子分析：

1. 为模型提供猫和狗的图像以及这些图像的标签，这些标签说明每张图像是猫还是狗
2. 一旦前向传递完成并且 cat 图像数据流经网络，模型将在最后提供输出，这将包括模型认为图像是什么，要么是猫，要么是狗
3. 从字面意义上讲，输出将由 cat 或 dog 的概率组成

### 损失和学习率

epoch

定义：指在训练期间将整个数据集单次传递到网络

一旦数据集中的所有数据点都通过网络，我们就说一个 epoch 完成了

#### Gradient of the loss function 损失函数的梯度

过程：

1. 计算损失后，将计算此损失函数相对于网络内每个权重的梯度。请注意，梯度只是几个变量的函数的导数的一个词
2. 我们已经计算了单个输出的损失，并计算了该损失相对于我们选择的单个权重的梯度。此计算是使用一种称为反向传播的技术完成的
3. 一旦我们有了损失函数的梯度值，我们就可以使用这个值来更新模型的权重。梯度告诉我们哪个方向将使损失趋向最小值，我们的任务是朝着降低损失的方向移动并更接近这个最小值

#### Learning rate  学习率

学习率是一个较小的数字，通常介于 0.01 和 0.0001 之间，但实际值可能会有所不同

学习率告诉我们应该朝着最小方向迈出多大的一步

#### Updating the weights  更新权重

![image-20250327202134886](Users/0.0/AppData/Roaming/Typora/typora-user-images/image-20250327202134886.png)

==注意==：

- 每次数据通过模型中的每个权重时，都会发生相同的过程，唯一的区别是，当计算损失函数的梯度时，每个权重的梯度值会有所不同，因为梯度是相对于每个权重计算的
- 所有这些权重都随着每个 epoch 的迭代更新。权重将逐渐越来越接近其优化值，而 SGD 将努力最小化损失函数

#### The model is learning

意义：权重的这种更新本质上就是我们说模型正在学习时的意思。它根据这些增量变化如何影响损失函数来学习为每个权重分配哪些值。随着权重的变化，网络在将输入准确映射到正确输出方面变得越来越智能

**使用 Keras 进行代码训练**

```python
import keras
from keras.models import Sequential
from keras.layers import Activation
from keras.layers.core import Dense
from keras.optimizers import Adam
from keras.metrics import categorical_crossentropy
model = Sequential([
    Dense(units=16, input_shape=(1,), activation='relu'),
    Dense(units=32, activation='relu'),
    Dense(units=2, activation='sigmoid')
])
model.compile(
    optimizer=Adam(learning_rate=0.0001),  # Adam为指定的优化器，是 SGD 的一个变体
    loss='sparse_categorical_crossentropy', 
    metrics=['accuracy']
)
model.fit(
    x=scaled_train_samples, # 由训练样本组成的 numpy 数组
    y=train_labels,  # numpy 数组，由训练样本的相应标签组成
    batch_size=10,  # 指定一次应向模型发送多少个训练样本
    epochs=20,  # 表示完整的训练集（所有样本）将总共传递给模型 20 次
    shuffle=True,  # 表示在将数据传递给模型之前，应先对数据进行 shuffle
    verbose=2 # 表示在模型训练时我们将看到多少日志记录
)
# 输出(epoch,持续时间，损失，准确率)
# 随着 epoch 的进行，损失正在下降，而准确性正在上升
Epoch 1/20 0s - loss: 0.6400 - acc: 0.5576
Epoch 2/20 0s - loss: 0.6061 - acc: 0.6310
Epoch 3/20 0s - loss: 0.5748 - acc: 0.7010
Epoch 4/20 0s - loss: 0.5401 - acc: 0.7633
Epoch 5/20 0s - loss: 0.5050 - acc: 0.7990
Epoch 6/20 0s - loss: 0.4702 - acc: 0.8300
Epoch 7/20 0s - loss: 0.4366 - acc: 0.8495
Epoch 8/20 0s - loss: 0.4066 - acc: 0.8767
Epoch 9/20 0s - loss: 0.3808 - acc: 0.8814
Epoch 10/20 0s - loss: 0.3596 - acc: 0.8962
Epoch 11/20 0s - loss: 0.3420 - acc: 0.9043
Epoch 12/20 0s - loss: 0.3282 - acc: 0.9090
Epoch 13/20 0s - loss: 0.3170 - acc: 0.9129
Epoch 14/20 0s - loss: 0.3081 - acc: 0.9210
Epoch 15/20 0s - loss: 0.3014 - acc: 0.9190
Epoch 16/20 0s - loss: 0.2959 - acc: 0.9205
Epoch 17/20 0s - loss: 0.2916 - acc: 0.9238
Epoch 18/20 0s - loss: 0.2879 - acc: 0.9267
Epoch 19/20 0s - loss: 0.2848 - acc: 0.9252
Epoch 20/20 0s - loss: 0.2824 - acc: 0.9286
```

### Loss in a Neural Network explained 神经网络中的损失解释

在神经网络编程中，给定样本的损失(模型的预测与 true 标签之间的差异)也称为误差

对于每个 epoch，误差在所有单独的 output 中累积

#### 均方误差 （MSE）

1. 对于单个样本，使用 MSE，我们首先计算提供的输出预测与标签之间的差异（误差）。然后我们调整此误差的平方

![image-20250327222638375](Users/0.0/AppData/Roaming/Typora/typora-user-images/image-20250327222638375.png)

2. 如果我们一次将多个样本传递给模型（一批样本），那么我们将取所有这些样本的平方误差的平均值
3. 如果我们一次将整个训练集传递给模型，那么我们刚刚计算损失的过程将在训练期间的每个 epoch 结束时发生
4. 如果我们将训练集拆分为多个批次，并一次将一个批次传递给我们的模型，那么将计算每个批次的损失

**使用 Keras 的代码中的损失函数**

```python
model = Sequential([
    Dense(16, input_shape=(1,), activation='relu'),
    Dense(32, activation='relu'),
    Dense(2, activation='sigmoid')
])
model.compile(
    Adam(learning_rate=.0001), 
    loss='sparse_categorical_crossentropy',  # 使用名为稀疏分类交叉熵的损失函数
    metrics=['accuracy']
)
```

Keras 当前可用的损失函数如下：

- mean_squared_error
- mean_absolute_error
- mean_absolute_percentage_error
- mean_squared_logarithmic_error
- squared_hinge
- hinge  
- categorical_hinge
- logcosh  
- categorical_crossentropy
- sparse_categorical_crossentropy
- binary_crossentropy
- kullback_leibler_divergence
- poisson  
- cosine_proximity

### 神经网络中的学习率解释

#### Learning rates and neural networks 学习率和神经网络

过程：

1. 从任意设置的权重开始训练过程，然后随着我们越来越接近最小化损失，逐渐更新这些权重
2. 为达到最小损失而采取的这些步骤的大小将取决于学习率。从概念上讲，可以将模型的学习率视为 *步长* 。
3. 在计算了输入的损失之后，会根据模型中的每个权重计算该损失的梯度。
4. 这些梯度的值就是我们学习率的来源。然后梯度将乘以学习率
5. 得到每个梯度的这个乘积的值乘以学习率，然后我们取这些值中的每一个，并通过从中减去这个值来更新相应的权重
6. 放弃之前在每个连接上设置的权重，并使用这些新值更新它们
7. 为学习率选择的值将需要一些测试。学习率是另一个因素 *超参数* 的一种，在我们确切知道要设置它的位置之前，我们必须对每个模型进行测试和优化，一个典型的准则是将其设置为 `0.01` 和 `0.0001`。
8. 权衡在较高的学习率和较低的学习率之间的选择

注意：在代码实现中，通常需要具有学习率的对象是优化

**使用 Keras 的代码中的损失函数**

```python
model = Sequential([
    Dense(units=16, input_shape=(1,), activation='relu'),
    Dense(units=32, activation='relu', kernel_regularizer=regularizers.l2(0.01)),
    Dense(units=2, activation='sigmoid')
])
# learning_rate 参数是可选的。如果没有明确设置，Keras 分配给这个特定优化器的默认学习率将是设置
model.compile(
    optimizer=Adam(learning_rate=0.0001), 
    loss='sparse_categorical_crossentropy', 
    metrics=['accuracy']
)
# 另一种方法是在编译完模型后，指定学习率
model.optimizer.learning_rate = 0.01
```

### 训练、测试和验证集解释

==数据集分类==：

- Training set  ==训练集==
- Validation set  ==验证集==
- Test set  ==测试集==

#### Training set  训练集

定义：用于训练模型的数据集

- 在每个 epoch 中，我们的模型将在训练集中的相同数据上一遍又一遍地进行训练，并且它将继续了解这些数据的特征
- 在单个 epoch 期间，训练集中的每个样本都会传递到网络
- 在神经网络的训练过程中，模型将对训练集和验证集中的每个输入进行分类。分类将*仅*基于网络从训练集中学到的数据
- 从而部署我们的模型，并让它准确预测以前从未见过的新数据。它将根据对训练数据的了解做出这些预测

#### Validation set  验证集

定义：一组独立于训练集的数据集，用于在训练期间验证模型(此验证过程有助于提供调整超参数的信息)

- 在训练期间的每个 epoch 中，模型都将根据训练集中的数据进行训练，同时在验证集中的数据上进行验证
- 在训练期间，模型也将对验证集中的每个输入进行分类。它将仅根据它对训练集中正在训练的数据的了解来执行此分类。模型中的权重不会根据我们的验证数据计算的损失进行更新
- 验证集中的数据与训练集中的数据是分开的。因此，当模型对此数据进行验证时，此数据不包含模型已经熟悉的训练样本
- 需要验证集的主要原因之一是确保我们的模型不会过度拟合训练集中的数据(过拟合的思路是，模型非常擅长对训练集中的数据进行分类，但它无法对未经过训练的数据进行泛化和准确分类)
- 在训练期间，如果我们也在验证集上验证模型，并看到它为验证数据提供的结果与它为训练数据提供的结果一样好，那么我们可以更确信我们的模型没有过度拟合

#### Test set  测试集

定义：用于在模型训练后测试模型的一组数据。测试集与训练集和验证集是分开的

- 使用训练集和验证集训练和验证模型后，]将使用模型来预测测试集中未标记数据的输出
- 测试集与其他两个测试集之间的一个主要区别是==测试集不应该被标记==。必须标记训练集和验证集，以便可以看到训练期间给出的指标，例如每个 epoch 的损失和准确率
- 当模型对测试集中的未标记数据进行预测时，这将与我们要将模型部署到现场时使用的相同类型的过程
- 在将模型部署到生产环境之前，测试集提供了模型是否泛化良好的最终检查

![image-20250327224829562](Users/0.0/AppData/Roaming/Typora/typora-user-images/image-20250327224829562.png)

### 